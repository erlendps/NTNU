# Eksamen Høst 2020

## oppg 1

Huffmans algoritme er en måte å komprimere data, og blir spesielt brukt til å gi alternative koder til strenger ved å se på frekvensen av hver karakter å gi en kode deretter. Istedenfor for at hver karakter f.eks trenger 4 bit for å lagres, kan Huffmans algoritme komprimere dette ved å la de karakterene som forekommer mest (har høyest frekvens) bli representert med færre bit, og dermed redusere minnestørrelsen. Huffman er en grådig algoritme som har optimal delstruktur og grådighetsegenskapen. Algoritmen går ut på at man har en min-heap (prioritetskø) der det minste elementet er den karakteren med lavest frekvens. Man henter ut de to karakterene med lavest karakter og lager en "gruppe" ut av disse, dvs summerer frekvensene, gir den med høyest frekvens av de to kode = 1 (og lavest 0) og legger denne nye gruppen tilbake til min-prioritetshaugen (man lager altså et sub-tre, der summen av frekvensene blir på en måte foreldernoden til de to spesifikke karakterene, og foreldrenoden blir representert med en gruppe, f.eks X). Deretter hentes på nytt de to laveste elementene fra haugen, og man fortsetter på samme måte til man har Huffmankodene sine.

## oppg 2

Når man velger ut kanter som skal bli med i et minimalt spenntre, skal kanten være "trygg". Det man gjør for å bestemme hvilken kant som er trygg er å lage et kutt i grafen og velge den "letteste" kanten som krysser dette kuttet. For at dette skal fungere må man anta noe mer om kantene. Kantene må ha en ikke-negativ vektfunksjon, samt at kantene må være urettet. Man må også velge kuttet litt spesielt. Hvis man lar A være et subsett av kantene til grafen G = (V, E) der kantene er inkludert i et minimalt spenntre for G, og så velger et hvilket som helst kutt (S, S-V) slik dette kuttet respekterer A (det vil si ingen av kantene i A krysser kuttet), så vil den letteste kanten som krysser dette kuttet være en trygg kant å legge til i A, altså vårt minimal spenntre. 
Dette vil være trygt siden kuttet (S, V-S) respekterer A (som vil representere det minimale spenntreet). Da vil ingen av kantene i A krysse kuttet. Når man da velger en kant som krysser dette kuttet er man garantert til å ikke ta med en kant som danner en sykel, nettop fordi det er ingen andre kanter i A som krysser kuttet. Dette er hvorfor det å velge kanten er riktig. For å lage et minimalt spenntre skal jo også kantvektene være minimale. Ved å velge grådig den kanten med lavest vekt viser det seg at dette vil danne et minimalt spenntre. Det gir også mening intuitivt, ettersom kan velge hvilket som helst kutt man vil som respekterer A, og det tilsier at kanten man velger ikke trenger å ha globalt minimum.

## oppg 3

For kjøretid bruker man gjerne asymptopisk notasjon. De mest vanlige er O(T(n)), \Sigma(T(n)) og \Omega(T(n)). Big-O notasjonen tilsier en øvre grense for hvor mye funksjonen vokser, f.eks betyr O(n²) at den øvre begrensingen på hvor fort kjøretiden vokser er på an² + bn + c, altså et andregradspolynom. Samtidig kan den også være bare n eller til og med 1/n. Big-Omega er tilsvarende, men gir en nedre grense. Det vil si at kjøretiden er garantert til å minst vokse med T(n). Sigma er en blanding av O og Omega. En kjøretid er Sigma hvis og bare hvis O(T(n)) = Omega(T(n)). Sigma garanterer altså at kjøretiden vil være T(n). 
Beste tilfellet er kjøretiden til algoritmen om den får beste mulig inputs. For eksepel for insertion sort så er beste tilfellet om listen allerede er sortert, og kjøretiden blir lineær. 
Gjennomsnittstilfellet er kjøretiden for et gjennomsnittlig input, det vil si om man gir algoritmen en rekke med tilfeldig inputs av lik størrelse. Dette er ikke like interessant alltid, ettersom det kan være vanskelig å definere hva et "gjennomsnittlig input" er. 
Verstetilfellet beregnes når algoritmen får verst mulig input, altså når den må gjøre mest antall steg for å terminere. Denne er ofte mest interssant å se på, ettersom man ofte er interessert i hvor lang tid algoritmen maksimalt vil ta. 
Man har også noe som heter amorisert analyse. En amorisert kjøretid er for én operasjon, og gir en øvre grense for hvor lang tid denne operasjonen tar i snitt. Altså gir gjennomsnittlige kjøretid for hver operasjon i verste tilfelle. 

## oppg 4

Metoden hennes ville fungert. Den opprinnelig algoritmen ville verste tilfelle blitt O(n²). Å bruke et binært søketre ville forbedret dette, ettersom det tar O(n*lg n) tid å konstruere et søketre (randomisert). Da vil operasjoner på treet slik som maksimum og delete som er interessant her ta O(lg(n)) tid. Siden man spør etter det største elementet n ganger, blir kjøretiden sigma(nlg(n)). Likevel er det litt unaturlig å bruke et søketre. Hvis man hadde kjørt inorder-tree-walk ville man fått en sortert liste, og man slipper å hente ut største element. En maks-heap hadde nok vært bedre. Siden vi da kan hente ut det største element i konstant tid, og max-heapify i lg(n) tid. Kjøretiden blir da Omega(n lg n), som er det samme som det med søketreet. Man kan selvfølelig få en dårlig heap (listen er allerede sortert, høyden på treet blir n), men i gjennomsnittlig tilfelle blir det Theta(n lg n). Hennes metode vil da tilsvare reversert Heap-sort, ettersom man henter ut høyeste element og plasserer det bakerst.

## oppg 5

Om man bruker en rekursiv løsningen på dette vil man få en eksponentiell kjøretid. Dette fordi vi kan se på hver unike sti fra toppen til bunnen som en rekursiv løsning. Det er veldig lett å se at dette da vil løse samme delinstans flere ganger. Kjøretiden blir faktisk 2^n. Dette kan enkelt forbedres ved å ta i bruk dynamisk programmering og bruke memorisering. Det vil si at vi tar vare på resultatet av hver delinstans, og når vi kommer til samme delinstans, så henter vi bare ut verdien lagret istedenfor å regne ut på nytt. Man kan da enten bruke top-down (rekursiv) eller bottom-up (iterativ). Med den iterative løsningen starter man fra "bunnen" og løser delproblemer iterativt, og lagrer resultatene. Med top-down bruker man rekusrsjon og memoriserer løsningen på delinstanser og henter de opp igjen når man møter på samme delinstans. For at dette skal fungere må problemstrukturen ha overlappende delinstanser. Det vil si at dersom man f.eks står mellom to valg, så vil man uavhengig av hvilken man velger, ende på samme delinstans etter hvert. Man må også ha en optimal delstruktur. Det vil si at for en optimal løsning på problemet, vil den optimale løsningen bestå av optimale løsninger på delproblemene.

## oppg 6

Quicksort-algoritmen består av å partisjonere et subsett ut ifra om elementene er større eller mindre enn et gitt pivotelement. I vanlig Quicksort så velges dette pivot-elementet til å være det siste elementet i listen. Dette kan gi problemer hvis input er valgt slik at partition-rutinen lager et subproblem av størrelse n-1 og et annet med 0 for hvert rekursive kall, som gir en kjøretid på Theta(n²). En måte å jobbe seg unna dette er å velge pivot-elementet tilfedlig. Dette gjøres ved at istedenfor at pivotelementet er det siste elementet i sbusettet, så blir det for hvert rekusrive kall valgt en tilfeldig indeks, som gir et tilfeldig pivotelement. Dette vil i gjennomsnitt skape balanserte partisjoner, det vil si ca. halvere størrelsen på problemene for hver gang.
Med randomized-select så jobber man kun rekursive kall på en partisjon, og ikke begge partisjonene. Til forskjell fra quicksort og randomized quicksort, så er faktisk select (som ikke bruker et tilfeldig element som pivot) garantert til å ha en lineær kjøretid i verste tilfelle. Så ved gjøre et deterministisk valg, forbedrer kjøretiden seg, i motsetning til quicksort/randomized-quicksort hvor å gjøre tilfeldige valg vil lønne seg.
Bildet kan endre seg. Ved å bruke select som subrutine kan man velge medianen som pivotelement, og du vil alltid få balanserte partisjoner. Siden vi da garantert partisjoner balansert (på midten), vil høyden på rekursjonstreet i quicksort være lg n. Og siden Select er garanert til å kjøre i lineær tid, vil kjøretiden i verste tilfelle da være O(n lg n).

## oppg 7

"Hvis A, så B" tilsvarer en implikasjon, altså en rettet kant. Vi kan derfor se på problemet som en rettet graf, der en kant fra A til B betyr at A utløste B. Grafen vil være uvektet (eller alle kanter vekt 1), så vi kan traverse denne grafen for å utforske hva som utløste hva. Siden man ønsker korte stier, er det naturlig å prøve å finne korteste vei. Et bredde-først-søk vil da passe. Hvis man kjører bredde-først fra noden man først satt til å være sann, f.eks X, vil man få korteste vei fra X til alle, og man får et traverseringstre som vil fortelle hva som utløste hva.

## oppg 8

For det første, vi vet at Edmons-Karp, en versjon av Ford-Fulkerson har en kjøretid på O(VE²), som er polynomisk. Vi kan redusere Ford-Fulkerson til Edmons-Karp, og siden vi har en løsning på Edmons-Karp har vi også en løsning på Ford-Fulkerson. Lurvik sier at maks-flyt egentlig er NP-hardt. Men siden det er polynomisk kjøretid, ville det implisert at P = NP, som er lite trolig. Reduseringen er heller ikke riktig vei. Lurvik virker å redusere maks-flyt til vektet hyperflyt, men reduseringen skal egentlig andre vei, fordi vi har en løsning på maks-flyt sier det ingenting om hyperflyt. 
Beckenbauch reduserer også feil vei. I hans reduksjon sier han at 3d-matching er minst like vanskelig som hyperflyt, som sier ingenting om hyperflyt. Reduksjonen skulle heller vært at man reduserer 3D-matching til hyperflyt, for da ville hyperflyt vært like vanskelig eller vanskeligere som 3D-matching, og 3D-matching er NP-hardt.