# Exam 2020

## Problem 1

```assembly
.text
main:
        ldr r4, =vars     ; r4 holds the start address of the vars
        ldr r0, [r4]      ; load a to r0
        ldr r1, [r4, #4]  ; load b to r1. int takes 4 bytes so offset by 4 bytes
        ldr r2, [r4, #8]  ; load c to r2
        ldr r3, [r4, #12] ; load d to r3
        ; all parameters are now in r0-r3 call calculate function
        bl calculate
        str r0, [r4, #12] ; returned value is in r0, store this in memory
        b end             ; branch to end

simple_calc:
        sub r0, r2, r3    ; calculate x - y
        bx lr             ; branch to callee

calculate:
        cmp r0, r1        ; check if a > b
        ble else          ; branch to else if it is less or equal
        add r0, r2, r3    ; place result of c + d in r0 (return register)
        b endif           ; branch to endif

else:
        push lr           ; nested function call, save the lr in stack
        bl simple_calc    ; branch and link to simple_calc
        pop lr            ; simple calc has returned, restore lr in stack
        b endif           ; branch to endif

endif:
        bx lr             ; branch back to main

end:

.data
vars:  
        .word 10  ; a
        .word 20  ; b
        .word 100 ; c
        .word 200 ; d
```

## Problem 2

The DMA stands for Direct Memory Access, and is a way for devices to communicate with the memory without having to ask the processor for permission to use the bus (and in general let the processor have control of the communcation). This allows for less delay between the device and memory since the device only has to get permission from the DMA to use the bus. The DMA asks the processor for permission to use the bus, which the processor can grant. Then communication is between the device, DMA and memory, excluding the processor. The DMA is far less complex than the processor, which makes it faster for it's purpose. Meanwhile, the processor can do other useful operations while the DMA controls the bus. It can for instance do operations on it's registers, or perhaps more likely, it fetches data from the cache, which it then performs operations on. This does not need access to the memory (and bus) as long as we get cache hits.

## Problem 3

A single-cycle processor does all the stages in one cycle. That means the processor fetches, decodes and executes an instruction all in one cycle (if there are 3 stages). This requires that the cycle time is large enough to fit all these stages, and we get a one instruction per cycle rate.

A fault with the single-cycle processor is that when it for example fetches an instruction, the other units are running idle, i.e doing nothing useful. A pipelined processor fixes this, by instead doing one stage per cycle, but it pipelines (runs in parallell) multiple instructions. For example in a 3-stage processor, during a single cycle, the fetch unit fetches the next instruction, the decode unit decodes the preceeding instruction (as it was fetched in the last cycle) and the execute unit executes the prepreceeding instruction (as it was decoded in the last cycle and fetched in the next to last cycle). When the pipeline is full, i.e all units are doing something, we still get 1 instruction per cycle, but since we are doing "less" each cycle, we can reduce the cycle time to give higher frequency, which means we execute more instructions per time unit.

A problem with is that bottlenecks easily can arise. For example we can have a slow memory when fetching the next instruction. It could for example take 2 cycles instead of one. Since the fetch unit cant fetch two instructions simultaionsusly the pipeline the next instruction has to wait for the next cycle. This is the case for any of the 3 stages. Another problem are conditonals and branching. In procedural execution the fetch unit is responsible for updating the PC, however, neither the fetch or decode unit know that we are about to branch. Only the execute unit know this when it actually executes. So if we branch in the execute unit, the fetch and decode unit are working on instructions that possibly might not be used. If it branches, the execute unit will update the PC. Also, the instructions that are currently being fetched and decoded has to be flushed. We then "lose" two cycles, and we don't have a 3x rate of instructions per time unit. Also the pipeline has to be filled up. At the start of the program, it fetches the first instruction, while decode and execute are idle. The next cycle it decodes and fetches a new instruction, while the excute is idle. At the third cycle all units are used, and we get maximum efficiency. This "cold" start has to be done each time instructions are flushed, or bottlenecked.

## Problem 4

It is a 4-way set-associative cache. That means that each set has 4 cache blocks each. There are 6 index-bits, which means there are $2^6 = 64$ sets. We then have $64 * 4 = 256$ blocks in the cache. There is used 6 bits for offset, so there are a total of $2^6 = 64$ bytes per block. This yields a cache with size $256 \* 64 = 16384$ bytes, equivalent to 16KB.

## Problem 5

Virtual memory gives the illusion to programs that they own the enitire main memory. The program has available a set of virtual space addresses. Virtual memory maps virtual addresses on the fly into physical addresses. The physical addresses points to a location in the main memory. The parts of the virtual address space that has not been used recently resides on disk, and not in memory, but are fetched into memory when needed. This tricks the programmer into thinking they own the entire memory. This also solves the issue when the virutal address space is larger than the main memory size, since unused virtual address space just resides on disk. This also allows several hundred programs to run simultanously, each program believing it has the entire main memory for itself.

Virtual memory also allows the OS to mange a user of one program access to other programs memory. It also makes it so none of the programs can meddle with the memory allocated by the OS (which is needed). It can do this by having permission bits for entries in e.g a page table if other users are allowed to read write or execute in this portion of the memory.

## Problem 6

The instruction region contains all the instructions that the program has, represented by machine code. The static data contains all global variables that are static during the program lifetime. The stack is reserved for extra parameters when calling a function, local variables, return addresses and for saving callee-saved registers, and there is a specific pattern to allocating and deallocating the stack (push, pop). The heap is reserved for dynamically allocated variables, things we don't know during compile time and only occurs during runtime. There is no specific pattern for allocating and deallocating memory in the heap.
The reason for stack growing downwards and heap upwards are mostly historical. When this concept was new, memory was very limited. And by having the stack grow downwards and heap upwards, both the stack and heap could live in the same area of the memory, since they are not overlapping until both are full.

## Problem 7

```c
int update_bit(int numA, int numB, int pos) {
  // error handling if position is greater or equal to 32, we will get overflow
  if (pos >= 32) {
    return -1;
  }
  // create a bit mask, left shifting one by pos
  int mask = 1 << pos;
  // check if bit at pos in numA is set
  // this will either be 0 (false) or a positve (true) result
  if (numA & mask) {
    // or-ing with the mask is the same as toggling bit as pos
    return numB | mask;
  }
  return numB;
}
```

## Problem 8

```c
// original
int q = 20;
int r = 11 - (q / 5);
int s;
s = r * 5;
if (s > 13) {
  s = r + 14
}
return s * (100 / q)

// after dead code elim
int q = 20;
int r = 11 - (q / 5);
int s;
s = r * 5;
if (s > 13) {
  s = r + 14
}
return s * (100 / q)

// after constant prop
int q = 20;
int r = 11 - (20 / 5); // q = 20
int s;
s = r * 5;
if (s > 13) {
  s = r + 14
}
return s * (100 / 20) // q = 20

// after constant folding
int q = 20;
int r = 7;  // 11 - (20/5) = 11 - 4 = 7
int s;
s = r * 5;
if (s > 13) {
  s = r + 14
}
return s * 5  // 100/20 = 5

// after dead code elim
// q is no longer needed
int r = 7; 
int s;
s = r * 5;
if (s > 13) {
  s = r + 14
}
return s * 5

// after const prop
int r = 7; 
int s;
s = 7 * 5; // r = 7
if (s > 13) {
  s = 7 + 14 // r = 7
}
return s * 5

// after const folding
int r = 7;
int s;
s = 35; // 7*5
if (s > 13) {
  s = 21 // 7 + 14
}
return s * 5

// after dead code elim
// r is no longer needed
int s;
s = 35;
if (s > 13) {
  s = 21
}
return s * 5

// after const prop
int s;
s = 35;
if (35 > 13) { // only s here can be propagated as we dont know what s*5 is
  s = 21;
}
return s * 5

// after const folding
int s;
s = 35;
// remove if, since it always evals to true
s = 21;
return s * 5

// after dead code elim
int s;
// remove s = 35 since it does not do anything
s = 21;
return s * 5

// after const prop
int s;
s = 21;
return 21 * 5 // s= 21

// after const folding
int s;
s = 21;
return 105 // 21 * 5 = 105

// after dead code elim
// s is not needed
return 105

// after const prop (nothing changes)
return 105

// after const fold (nothing changes)
return 105

// dead code elim (nothing changes)
return 105
```

## Problem 9

1 ) For a process to request I/O in this simple model, it has to be in the running state. When it requests the I/O it does a I/O requests transition into the blocked state. The OS then dispatches a process in the ready state.

2 ) The process then does a I/O completion transition into the ready state, indicating to the OS that it is ready to be execute again. Dependent on the scheduling the OS implements it will either wait in the ready state until it is dispatched, or dispatched immediatly (if for instance it has a high priority).

3 ) When a timer interrupt occurs, the process currently in the RUNNING state has not finished executing, and is timeouted back into the ready state. Meanwhile, another process (or the same, depending on implementation) is dispatched to the running state, and starts execution.

## Problem 10

Both clock gating and power gating are hardware implementations of power saving. Clock gating means to basically turn of the clock, i.e the clock always gives 0. This saves dynamic power since we never transition from 1 state to another in the CMOS. By removing the clock, the frequency becomes 0 and the dynamic power consumption will be 0. This does not have affect on the static power consumption. Power gating is when you turn off the current in for a IC. This reduces both dynamic and static power consumption. Static because you turn off the current, so there can be no leakage, and dynamic since by turning off the current we don't have any voltage, so no switching can occur.
